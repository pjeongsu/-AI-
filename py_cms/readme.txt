# 상황 :
    - 웹 개발이 어느정도 되었다
    - 디자인이 모두 완성이 되었고, 디자인 파일(이미지,html,css)등을
    전달받았다
    - 내가 만든 각각의 페이지 코드에 디자인을 입힌다

# 구조 :
    root
    L run.py     : 엔트리포인트, 웹서비스 가동시 구동하는 파일
    L raedme.txt : 본 프로젝트에 대한 설명, 주의사항 등등
    L templates  : 모든 html 파일이 위치한 곳, render_tamplate() 기본적으로 인식하는 디렉토리
    L static     : 모든 정적 파일이 위치하는 곳
                   이미지, css, js, 기타 파일들
                 : 라우팅 없이 경로를 인식한다. 정적 파일만 존재한다 ( 보안적으로 위험)
        L *.js
        L *.css
        L *.png,..

# 금주 내용 (월/화)
- flask
    - 배포 및 운영
        - aws EC2 서버 구축(리눅스(우분투 18.04))
            - EC2에 생성된 리눅스 서버에 접속
                - 리눅스, 맥 => 바로 터미널에서 가이드대로 접속
                - 윈도우
                    => putty와 같은 ssh에 접속이 가능한 s/w을 이용
                    => 윈도우 openssh라는 프로그램 설치한 후 
                        => powershell에서 접속
        - local pc => 아마존 클라우드 서버로 배포(deploy)
            - fabric 라이브러리 사용
            - 업데이트, 서버 가능
            - 절차 
                - 1. 개발
                - 2. github 관리(최신버전, 작업파일을 매일 백업,...)
                - 3. 배포
                    - 3-1. 서버 구축
                    - 3-2. 개발자가 서버에 코드를 배포
                        - 3-2-1. fablic 작성(로컬PC에서 원격PC에 연결하여, 세팅, 업데이트 등등)
                        - 3-2-2. 구동 => 서버에 최신버전으로 반영이 된다 => 서비스 가동
        - github 가입이 되어 있어야 한다
            - 소스 컨트롤
            - 날짜별로 관리, 버전 관리 등등 ...
    - 고급화
        - 구조를 Django 수준으로 구성하여 관리
        - 아키텍쳐 구성, 디비 접속부분 고급화(풀링처리, orm)
        - 요청과 응답에 대한 라이프사이클
            - 디테일한 관리
        - url 관리를 통한 업무 분담 
            - 블루프린트 기술 이용
    - 기타
        - 기능 : 웹소캣, socket, io 등등 이용한 실시간 통신, 자료실(파일업로드), 쿠키
        - 웹 소캣 사용 설치 
            - 서버
                - pip(or conda) install flask-socketIO
            - 클라이언트
                - socket.io의 CDN
                    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/1.7.3/socket.io.min.js"></script>

# 데이터 수집(수~금)
    - 데이터 사이언스 과정
        - 1. 요구사항 분석
        - 2. 데이터 수집
            - 난이도에 따라 구성
            - lv1
                - 데이터를 제공받는 케이스
                    - 공공데이터, 공모전 데이터(활용x), 연구기관/교육기관 제공
                    - 사내데이터(회사 내부 데이터)
            ----------------- 이하는 웹에서 구할 경우 --------------------------------
            - lv2
                - open API가 존재하는 케이스
                - 인증키를 통해서, 하루에 적정량의 데이터를 질의하여 활용할 수 있는 경우
            - lv3
                - 해당 웹 페이지에서 바로 데이터를 수집할 수 있다면?
                - Web Scrapping(웹 스크래핑)
                - request, bs4(beautiful soup)
            - lv4
                - 해당 웹 페이지가 사용자와 인터렉션을 통해서(반응해서) 데이터가 노출된다
                - 더보기, 스크롤, 로그인, 검색 등등 케이스, ajax를 사용한 사이트
                - selenium(셀레니움) + 웹 드라이버(브라우저 회사별로 제공하는)
            + 
            - 자동화
                - os 레벨에서 자동으로 데이터를 수집하게 하는 활동을 작성 / 운용
                - lv3/lv4 같은 경우는 단시간에 빠른 접속을 지속적으로 시도하면 
                    - 디도스로 간주하 ㄹ수 있기 때문에 적절한 시간 조절
                    - 고급(접속한 유저의 iip를 우회하여(플락시 서버) 처리)

        - 3. 데이터 준비/전처리/적제
        - 4. 데이터 분석
        - 5. 모델 구축
        - 6. 시스템 구축/서비스 구성/레포트 => 산출물
